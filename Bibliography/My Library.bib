
@article{cooper_perceptual_2023,
	title = {The Perceptual Science of Augmented Reality},
	volume = {9},
	issn = {2374-4642, 2374-4650},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-vision-111022-123758},
	doi = {10.1146/annurev-vision-111022-123758},
	abstract = {Augmented reality ({AR}) systems aim to alter our view of the world and enable us to see things that are not actually there. The resulting discrepancy between perception and reality can create compelling entertainment and can support innovative approaches to education, guidance, and assistive tools. However, building an {AR} system that effectively integrates with our natural visual experience is hard. {AR} systems often suffer from visual limitations and artifacts, and addressing these flaws requires basic knowledge of perception. At the same time, {AR} system development can serve as a catalyst that drives innovative new research in perceptual science. This review describes recent perceptual research pertinent to and driven by modern {AR} systems, with the goal of highlighting thought-provoking areas of inquiry and open questions.},
	pages = {455--478},
	issue = {Volume 9, 2023},
	journaltitle = {Annual Review of Vision Science},
	author = {Cooper, Emily A.},
	urldate = {2024-12-12},
	date = {2023-09-15},
	langid = {english},
	note = {Publisher: Annual Reviews},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\225WKHFA\\Cooper - 2023 - The Perceptual Science of Augmented Reality.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\XWFXQ4W7\\annurev-vision-111022-123758.html:text/html},
}

@article{spiegel_16-3_2024,
	title = {16-3: Distinguished Paper: Vergence-Accommodation Conflict Increases Time to Focus in Augmented Reality},
	volume = {55},
	issn = {2168-0159},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sdtp.17485},
	doi = {10.1002/sdtp.17485},
	shorttitle = {16-3},
	abstract = {Vergence-Accommodation Conflicts ({VAC}) occur in neareye displays when the binocular disparity of the 3D rendered content (vergence) does not match the display focal distance (accommodation). {VAC} has been shown to reduce perceptual image quality, cognitive performance, and oculomotor coordination. In this study, we aimed to investigate the impact of {VAC} on visual performance in augmented reality ({AR}). Specifically, we quantified the impact of {AR} {VAC} on the ‘Time to Focus’ ({TTF}); when the user switches focus between real world content and worldlocked {AR} rendered content. Our results show that {TTF} increases exponentially with {VAC}. The increase is more pronounced at closer vergence distances in displays with focal distance of 1 D or longer. Finally, we showed that {VAC} may have a differential effect across age groups; specifically, older users may be affected more in closer focal and longer vergence distances 186-189.},
	pages = {186--189},
	number = {1},
	journaltitle = {{SID} Symposium Digest of Technical Papers},
	author = {Spiegel, Daniel P. and Erkelens, Ian M.},
	urldate = {2024-12-10},
	date = {2024},
	langid = {english},
	note = {\_eprint: https://sid.onlinelibrary.wiley.com/doi/pdf/10.1002/sdtp.17485},
	keywords = {{AR}, fixed focus display, head-mount display, near-eye display, presbyopia, stereoscopic display, {VAC}, virtual reality, visual comfort, Visual performance, {VR}, Zone of Normal Performance, Zone of Reduced Performance},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\VZ56U2I8\\Spiegel and Erkelens - 2024 - 16-3 Distinguished Paper Vergence-Accommodation Conflict Increases Time to Focus in Augmented Real.pdf:application/pdf;J Soc Info Display - 2024 - Spiegel - Vergence‐accommodation conflict increases time to focus in augmented reality:C\:\\Users\\annab\\Zotero\\storage\\3SQW3IW8\\J Soc Info Display - 2024 - Spiegel - Vergence‐accommodation conflict increases time to focus in augmented reality.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\TI4FAM8Q\\sdtp.html:text/html},
}

@article{spiegel_vergence-accommodation_2024,
	title = {Vergence-accommodation conflicts and low additive contrast increase time to focus in augmented reality},
	volume = {65},
	issn = {1552-5783},
	abstract = {Vergence-accommodation conflicts ({VAC}) occur when the visual system receives a mismatch between vergence and accommodation cues. In augmented reality ({AR}), {VAC} mainly manifests when switching focus between real world and {AR} content, and is a major challenge for {AR} devices. In our study, we developed a novel metric, time to focus ({TTF}), and quantified the impact of {VAC} and additive contrast on visual performance in {AR}.    Sixteen participants (21 to 41 years; mean 29.4 ± 5.8 {SD}) with corrected-to-normal vision participated in the study. Visual stimuli were generated by a three-plane variable focus haploscope. Each trial consisted of two intervals and simulated switching focus from real world to {AR} content. In the first interval, we presented a 10 arcmin Landolt C on 1/f noise in the center of the visual field. Its focal and vergence distances were matched (no {VAC}) and set to 1D. In the second interval, another Landolt C with flankers appeared above or below the first C. The vergence distance of the second C was also 1D; however, its focal distance varied between 2.5D and 0D to induce variable degrees of {VAC} (including 1D as a no {VAC} control condition). In addition, the second C also varied in additive contrast ranging from 1.6:1 to 12:1. The task in both intervals was to identify the orientation of the C opening in four cardinal directions. The duration of the first interval was unlimited and a correct response triggered the second interval. The duration of the second interval was varied by an adaptive staircase to determine the {TTF}. The {TTFs} were normalized as \% change from the control no {VAC} condition.    The main finding was that the {TTF} exponentially increased as a function of {VAC} (effect of {VAC} in {ANOVA} F6,90 = 18.1, p \&lt; 0.001). Furthermore, the {TTF} increase was exacerbated by reducing the additive contrast (interaction between {VAC} and contrast F18,270 = 5.8, p \&lt; 0.001). This trend was most pronounced at closer vergence distances reaching an average of 156.3\% (± 33.7) increase in {TTF} at 2.5 D vergence distance.    In this study we developed a novel metric to quantify the impact of {AR} {VAC} and additive on visual performance. We showed that {VAC} increases {TTF} which is further exacerbated by reduced additive contrast. More research is warranted to understand how these observations contribute to visual fatigue associated with usage of augmented reality devices.  This abstract was presented at the 2024 {ARVO} Annual Meeting, held in Seattle, {WA}, May 5-9, 2024.},
	pages = {6526},
	number = {7},
	journaltitle = {Investigative Ophthalmology \& Visual Science},
	shortjournal = {Investigative Ophthalmology \& Visual Science},
	author = {Spiegel, Daniel P and Erkelens, Ian},
	date = {2024-06-17},
	file = {Snapshot:C\:\\Users\\annab\\Zotero\\storage\\SNXRWQAY\\article.html:text/html},
}

@article{mercier_fast_2017,
	title = {Fast gaze-contingent optimal decompositions for multifocal displays},
	volume = {36},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3130800.3130846},
	doi = {10.1145/3130800.3130846},
	abstract = {As head-mounted displays ({HMDs}) commonly present a single, fixed-focus display plane, a conflict can be created between the vergence and accommodation responses of the viewer. Multifocal {HMDs} have long been investigated as a potential solution in which multiple image planes span the viewer's accommodation range. Such displays require a scene decomposition algorithm to distribute the depiction of objects across image planes, and previous work has shown that simple decompositions can be achieved in real-time. However, recent optimal decompositions further improve image quality, particularly with complex content. Such decompositions are more computationally involved and likely require better alignment of the image planes with the viewer's eyes, which are potential barriers to practical applications.
            Our goal is to enable interactive optimal decomposition algorithms capable of driving a vergence- and accommodation-tracked multifocal testbed. Ultimately, such a testbed is necessary to establish the requirements for the practical use of multifocal displays, in terms of computational demand and hardware accuracy. To this end, we present an efficient algorithm for optimal decompositions, incorporating insights from vision science. Our method is amenable to {GPU} implementations and achieves a three-orders-of-magnitude speedup over previous work. We further show that eye tracking can be used for adequate plane alignment with efficient image-based deformations, adjusting for both eye rotation and head movement relative to the display. We also build the first binocular multifocal testbed with integrated eye tracking and accommodation measurement, paving the way to establish practical eye tracking and rendering requirements for this promising class of display. Finally, we report preliminary results from a pilot user study utilizing our testbed, investigating the accommodation response of users to dynamic stimuli presented under optimal decomposition.},
	pages = {1--15},
	number = {6},
	journaltitle = {{ACM} Transactions on Graphics},
	shortjournal = {{ACM} Trans. Graph.},
	author = {Mercier, Olivier and Sulai, Yusufu and Mackenzie, Kevin and Zannoli, Marina and Hillis, James and Nowrouzezahrai, Derek and Lanman, Douglas},
	urldate = {2024-12-10},
	date = {2017-12-31},
	langid = {english},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\TNXTTM8Z\\Mercier et al. - 2017 - Fast gaze-contingent optimal decompositions for multifocal displays.pdf:application/pdf},
}

@article{jia_local_2023,
	title = {Local pupil swim in Virtual- and Augmented-Reality: Root cause and perception model},
	volume = {31},
	rights = {© 2023 Society for Information Display.},
	issn = {1938-3657},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jsid.1210},
	doi = {10.1002/jsid.1210},
	shorttitle = {Local pupil swim in Virtual- and Augmented-Reality},
	abstract = {The optics between display and human eye in a typical {VR}/{AR} headmounted display ({HMD}) can introduce a common visual defect - local pupil swim (also called local ripples or “orange peel” effect), where virtual content distorts locally with head movement. Compact optical design (such as pancake optics) is increasingly sensitive in design and manufacturing tolerance to this perceptual effect. This work provides a method to root cause and quantify the impact based on perceptual modeling, optics simulation, and measurement.},
	pages = {230--240},
	number = {5},
	journaltitle = {Journal of the Society for Information Display},
	author = {Jia, Jerry and Chan, Tsz Tai and Lian, Trisha and Rio, Kevin W.},
	urldate = {2024-12-10},
	date = {2023},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jsid.1210},
	keywords = {{VR}, distortion, perception, pupil swim, visual ripples},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\DF6SAHW5\\Jia et al. - 2023 - Local pupil swim in Virtual- and Augmented-Reality Root cause and perception model.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\3UZRA5UU\\jsid.html:text/html},
}

@thesis{estemyr_designing_2023,
	title = {Designing a {VR} user experience test regarding the Vergence-Accommodation Conflict : An investigation surrounding the relations to Depth Perception},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:bth-25168},
	shorttitle = {Designing a {VR} user experience test regarding the Vergence-Accommodation Conflict},
	abstract = {This bachelor project investigates how to design a virtual reality ({VR}) user experience test to  analyze the effect that the vergence-accommodation conflict ({VAC}) has on a users ability to  judge focus. Furthermore, we want to investigate how the amount of depth cues can affect a  user's perception of this conflict in {VR}. A user experience test has been designed through an  iterative process where the prototype has gone through smaller user tests between each  iteration. We validate the design through testing a larger number of participants, where we  gather subjective data through the use of the “Think Aloud Method” as well as data based on  the user's interactions within the test environment. Analyzed results show that the absence of  depth cues makes it significantly harder for users to judge depth and focus, and that the  presence of an additional reference-object in many cases is enough to assist users with this  issue. Furthermore, we notice better user performance at closer distances. The aim of this  investigation is to provide more information surrounding the perception of {VAC}, as it is one  of the few issues within the {VR} industry that prevents us from experiencing depth in virtual  worlds in a more realistic way.},
	type = {phdthesis},
	author = {Estemyr, Emil and Ekhagen, Alexander},
	urldate = {2024-12-10},
	date = {2023},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\73JJFVDE\\Estemyr and Ekhagen - 2023 - Designing a VR user experience test regarding the Vergence-Accommodation Conflict  An investigation.pdf:application/pdf},
}

@thesis{zhang_lightness_2022,
	location = {United States -- New York},
	title = {Lightness, Brightness, and Transparency in Optical See-Through Augmented Reality},
	rights = {Database copyright {ProQuest} {LLC}; {ProQuest} does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/docview/2648211984/abstract/B377F5DB8D0B43EDPQ/1},
	abstract = {Augmented reality ({AR}), as a key component of the future metaverse, has leaped from the research labs to the consumer and enterprise markets. {AR} optical see-through ({OST}) devices utilize transparent optical combiners to provide visibility of the real environment as well as superimpose virtual content on top of it. {OST} displays distinct from existing media because of their optical additivity, meaning the light reaching the eyes is composed of both virtual content and real background. The composition results in the intended virtual colors being distorted and perceived transparent. When the luminance of the virtual content decreases, the perceived lightness and brightness decrease, and the perceived transparency increases. Lightness, brightness, and transparency are modulated by one physical dimension (luminance), and all interact with the background and each other. In this research, we aim to identify and quantify the three perceptual dimensions, as well as build mathematical models to predict them.
In the first part of the study, we focused on the perceived brightness and lightness with two experiments: a brightness partition scaling experiment to build brightness scales, and a diffuse white adjustment experiment to determine the absolute luminance level required for diffuse white appearances on 2D and 3D {AR} stimuli. The second part of the research targeted at the perceived transparency in the {AR} environment with three experiments. The transparency was modulated by the background Michelson contrast reduction in either average luminance or peak-to-peak luminance difference to investigate, and later illustrated, the fundamental mechanism evoking transparency perception. The first experiment measured the transparency detection thresholds and confirmed that contrast sensitivity functions with contrast adaptation could model the thresholds. Subsequently, the transparency perception was investigated through direct anchored scaling experiment by building perceived transparency scales from the virtual content contrast ratio to the background. A contrast-ratio-based model was proposed predicting the perceived transparency scales. Finally, the transparency equivalency experiment between the two types of contrast modulation confirmed the mechanism difference and validated the proposed model.},
	pagetotal = {183},
	institution = {Rochester Institute of Technology},
	type = {phdthesis},
	author = {Zhang, Lili},
	urldate = {2024-12-10},
	date = {2022},
	note = {{ISBN}: 9798209939672},
	keywords = {Augmented reality, Brightness, Cognitive psychology, Experimental psychology, Lightness, Psychology, See-through, Transparency},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\UC668XWH\\Zhang - 2022 - Lightness, Brightness, and Transparency in Optical See-Through Augmented Reality.pdf:application/pdf},
}

@article{pacchierotti_guest_2024,
	title = {Guest Editorial Haptics in the Metaverse: Haptic Feedback for Virtual, Augmented, Mixed, and {eXtended} Realities},
	volume = {17},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1939-1412, 2329-4051, 2334-0134},
	url = {https://ieeexplore.ieee.org/document/10566110/},
	doi = {10.1109/TOH.2024.3369192},
	shorttitle = {Guest Editorial Haptics in the Metaverse},
	pages = {122--128},
	number = {2},
	journaltitle = {{IEEE} Transactions on Haptics},
	shortjournal = {{IEEE} Trans. Haptics},
	author = {Pacchierotti, Claudio and Chinello, Francesco and Koumaditis, Konstantinos and Luca, Massimiliano Di and Ofek, Eyal and Georgiou, Orestis},
	urldate = {2024-12-10},
	date = {2024-04},
	langid = {english},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\3RBQT2T3\\Pacchierotti et al. - 2024 - Guest Editorial Haptics in the Metaverse Haptic Feedback for Virtual, Augmented, Mixed, and eXtende.pdf:application/pdf},
}

@inproceedings{schubert_intuitive_2023,
	location = {New York, {NY}, {USA}},
	title = {Intuitive User Interfaces for Real-Time Magnification in Augmented Reality},
	isbn = {979-8-4007-0328-7},
	url = {https://dl.acm.org/doi/10.1145/3611659.3615694},
	doi = {10.1145/3611659.3615694},
	series = {{VRST} '23},
	abstract = {Various reasons exist why humans desire to magnify portions of our visually perceived surroundings, e.g., because they are too far away or too small to see with the naked eye. Different technologies are used to facilitate magnification, from telescopes to microscopes using monocular or binocular designs. In particular, modern digital cameras capable of optical and/or digital zoom are very flexible as their high-resolution imagery can be presented to users in real-time with displays and interfaces allowing control over the magnification. In this paper, we present a novel design space of intuitive augmented reality ({AR}) magnifications where an {AR} head-mounted display is used for the presentation of real-time magnified camera imagery. We present a user study evaluating and comparing different visual presentation methods and {AR} interaction techniques. Our results show different advantages for unimanual, bimanual, and situated {AR} magnification window interfaces, near versus far vergence distances for the image presentation, and five different user interfaces for specifying the scaling factor of the imagery.},
	pages = {1--10},
	booktitle = {Proceedings of the 29th {ACM} Symposium on Virtual Reality Software and Technology},
	publisher = {Association for Computing Machinery},
	author = {Schubert, Ryan and Bruder, Gerd and Welch, Greg},
	urldate = {2024-12-10},
	date = {2023-10-09},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\4VBJS4HG\\Schubert et al. - 2023 - Intuitive User Interfaces for Real-Time Magnification in Augmented Reality.pdf:application/pdf},
}

@article{peereboom_head-locked_2024,
	title = {Head-locked, world-locked, or conformal diminished-reality? An examination of different {AR} solutions for pedestrian safety in occluded scenarios},
	volume = {28},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-024-01017-9},
	doi = {10.1007/s10055-024-01017-9},
	shorttitle = {Head-locked, world-locked, or conformal diminished-reality?},
	abstract = {Many collisions between pedestrians and cars are caused by poor visibility, such as occlusion by a parked vehicle. Augmented reality ({AR}) could help to prevent this problem, but it is unknown to what extent the augmented information needs to be embedded into the world. In this virtual reality experiment with a head-mounted display ({HMD}), 28 participants were exposed to {AR} designs, in a scenario where a vehicle approached from behind a parked vehicle. The experimental conditions included a head-locked live video feed of the occluded region, meaning it was fixed in a specific location within the view of the {HMD} ({VideoHead}), a world-locked video feed displayed across the street ({VideoStreet}), and two conformal diminished reality designs: a see-through display on the occluding vehicle ({VideoSeeThrough}) and a solution where the occluding vehicle has been made semi-transparent ({TransparentVehicle}). A Baseline condition without augmented information served as a reference. Additionally, the {VideoHead} and {VideoStreet} conditions were each tested with and without the addition of a guiding arrow indicating the location of the approaching vehicle. Participants performed 42 trials, 6 per condition, during which they had to hold a key when they felt safe to cross. The keypress percentages and responses from additional questionnaires showed that the diminished-reality {TransparentVehicle} and {VideoSeeThrough} designs came out most favourably, while the {VideoHead} solution caused some discomfort and dissatisfaction. An analysis of head yaw angle showed that {VideoHead} and {VideoStreet} caused divided attention between the screen and the approaching vehicle. The use of guiding arrows did not contribute demonstrable added value. {AR} designs with a high level of local embeddedness are beneficial for addressing occlusion problems when crossing. However, the head-locked solutions should not be immediately dismissed because, according to the literature, such solutions can serve tasks where a salient warning or instruction is beneficial.},
	pages = {119},
	number = {2},
	journaltitle = {Virtual Reality},
	shortjournal = {Virtual Reality},
	author = {Peereboom, Joris and Tabone, Wilbert and Dodou, Dimitra and de Winter, Joost},
	urldate = {2024-12-10},
	date = {2024-05-27},
	langid = {english},
	keywords = {Augmented reality, Artificial Intelligence, Assisted reality, Diminished reality, Local presence, Pedestrian safety, Virtual reality experiment},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\3ZNSXSV4\\Peereboom et al. - 2024 - Head-locked, world-locked, or conformal diminished-reality An examination of different AR solutions.pdf:application/pdf},
}

@article{carnegie_reducing_2015,
	title = {Reducing Visual Discomfort with {HMDs} Using Dynamic Depth of Field},
	volume = {35},
	issn = {1558-1756},
	url = {https://ieeexplore.ieee.org/abstract/document/7274431?casa_token=UaIgC8ByHXUAAAAA:0yQlxcCaOHvIGSzZdQg6sboYmyKxC_YFdEyHME6SCYlAa0NCM_GpjkHMvqvRALtWKvFgwFJR},
	doi = {10.1109/MCG.2015.98},
	abstract = {Although head-mounted displays ({HMDs}) are ideal devices for personal viewing of immersive stereoscopic content, exposure to {VR} applications on them results in significant discomfort for the majority of people, with symptoms including eye fatigue, headaches, nausea, and sweating. A conflict between accommodation and vergence depth cues on stereoscopic displays is a significant cause of visual discomfort. This article describes the results of an evaluation used to judge the effectiveness of dynamic depth-of-field ({DoF}) blur in an effort to reduce discomfort caused by exposure to stereoscopic content on {HMDs}. Using a commercial game engine implementation, study participants report a reduction of visual discomfort on a simulator sickness questionnaire when {DoF} blurring is enabled. The study participants reported a decrease in symptom severity caused by {HMD} exposure, indicating that dynamic {DoF} can effectively reduce visual discomfort.},
	pages = {34--41},
	number = {5},
	journaltitle = {{IEEE} Computer Graphics and Applications},
	author = {Carnegie, Kieran and Rhee, Taehyun},
	urldate = {2024-12-10},
	date = {2015-09},
	note = {Conference Name: {IEEE} Computer Graphics and Applications},
	keywords = {virtual reality, computer graphics, depth of field, Displays, Hardware, head mounted display, immersive stereoscopic content, Software, Stereo image processing, Virtual reality, Visual systems, Visualization},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\annab\\Zotero\\storage\\FAQ9WJ7W\\7274431.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\LX4CWZSZ\\Carnegie and Rhee - 2015 - Reducing Visual Discomfort with HMDs Using Dynamic Depth of Field.pdf:application/pdf},
}

@article{sun_visual_2022,
	title = {Visual discomfort factor analysis and modeling for worldwide stereoscopic 3D maps},
	volume = {75},
	issn = {0141-9382},
	url = {https://www.sciencedirect.com/science/article/pii/S0141938222001019},
	doi = {10.1016/j.displa.2022.102281},
	abstract = {{EarthView}3D is the first worldwide orthographic stereoscopic 3D (S3D) map. It presents an accurate presentation of the Earth’s surface and can provide an immersive geospatial infrastructure for the upcoming metaverse ecosystems. This study conducted the first comprehensive assessment of the worldwide S3D maps, investigated the factors of visual discomfort, predicted discomfort level, and provided design recommendations for improving visual experience. Participants rated the S3D map images and reported the reasons for their visual discomfort. General Eye Symptom Questionnaires ({GESQs}) and Simplified Simulator Sickness Questionnaires ({SSSQs}) were used to record the development of visual fatigue with time. Six categories of measurable variables presumably related to visual discomfort were proposed based on participants’ reports, {IEEE} standards, and literature on visual experience. Factor analysis extracted four principal factors, disparity, terrain texture, luminance, and amplitude spectrum, whose contributions to variance were 38\%, 23\%, 16\%, and 11\%, respectively. The selected variables and subjective mean rating scores were used to construct a regression model for the prediction of visual discomfort. Performance evaluation Root Mean Square Error ({RMSE}) was lower than that reported in previous studies on different S3D databases. The results indicated that besides vergence-accommodation conflict and depth cue conflict, perception of Earth terrain’s textures and luminance played an important role in visual discomfort of viewing orthographic S3D maps.},
	pages = {102281},
	journaltitle = {Displays},
	shortjournal = {Displays},
	author = {Sun, Ganyun and Liu, Weilong and Zhang, Yun and Fraser, David},
	urldate = {2024-12-10},
	date = {2022-12-01},
	keywords = {Amplitude Spectrum, Edge Violation, Factor Analysis, Horizontal Disparity, Machine Learning Regression, Stereoscopic 3D Maps, Terrain Texture, Visual Comfort, Visual Saliency},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\QPZ2YKYD\\Sun et al. - 2022 - Visual discomfort factor analysis and modeling for worldwide stereoscopic 3D maps.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\annab\\Zotero\\storage\\EXNIER95\\S0141938222001019.html:text/html},
}

@article{dey_lessons_2014,
	title = {Lessons learned: Evaluating visualizations for occluded objects in handheld augmented reality},
	volume = {72},
	issn = {1071-5819},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581914000652},
	doi = {10.1016/j.ijhcs.2014.04.001},
	shorttitle = {Lessons learned},
	abstract = {Handheld devices like smartphones and tablets have emerged as one of the most promising platforms for Augmented Reality ({AR}). The increased usage of these portable handheld devices has enabled handheld {AR} applications to reach the end-users; hence, it is timely and important to seriously consider the user experience of such applications. {AR} visualizations for occluded objects enable an observer to look through objects. {AR} visualizations have been predominantly evaluated using Head-Worn Displays ({HWDs}), handheld devices have rarely been used. However, unless we gain a better understanding of the perceptual and cognitive effects of handheld {AR} systems, effective interfaces for handheld devices cannot be designed. Similarly, human perception of {AR} systems in outdoor environments, which provide a higher degree of variation than indoor environments, has only been insufficiently explored. In this paper, we present insights acquired from five experiments we performed using handheld devices in outdoor locations. We provide design recommendations for handheld {AR} systems equipped with visualizations for occluded objects. Our key conclusions are the following: (1) Use of visualizations for occluded objects improves the depth perception of occluded objects akin to non-occluded objects. (2) To support different scenarios, handheld {AR} systems should provide multiple visualizations for occluded objects to complement each other. (3) Visual clutter in {AR} visualizations reduces the visibility of occluded objects and deteriorates depth judgment; depth judgment can be improved by providing clear visibility of the occluded objects. (4) Similar to virtual reality interfaces, both egocentric and exocentric distances are underestimated in handheld {AR}. (5) Depth perception will improve if handheld {AR} systems can dynamically adapt their geometric field of view ({GFOV}) to match the display field of view ({DFOV}). (6) Large handheld displays are hard to carry and use; however, they enable users to better grasp the depth of multiple graphical objects that are presented simultaneously.},
	pages = {704--716},
	number = {10},
	journaltitle = {International Journal of Human-Computer Studies},
	shortjournal = {International Journal of Human-Computer Studies},
	author = {Dey, Arindam and Sandor, Christian},
	urldate = {2024-12-10},
	date = {2014-10-01},
	keywords = {Augmented reality, Experiments, Handheld devices, Outdoor environments, Visualizations for occluded objects},
	file = {Dey2014_LessonsLearnedEvaluatingVisualizationsForoccludedObjectsInHandheldAR:C\:\\Users\\annab\\Zotero\\storage\\QZM8LDTI\\Dey2014_LessonsLearnedEvaluatingVisualizationsForoccludedObjectsInHandheldAR.pdf:application/pdf;PDF:C\:\\Users\\annab\\Zotero\\storage\\JHCGXNBN\\Dey and Sandor - 2014 - Lessons learned Evaluating visualizations for occluded objects in handheld augmented reality.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\annab\\Zotero\\storage\\VKMSLNKG\\S1071581914000652.html:text/html},
}

@article{soomro_visual_2020,
	title = {Visual acuity response when using the 3D head-up display in the presence of an accommodation-convergence conflict},
	volume = {21},
	issn = {1598-0316},
	url = {https://doi.org/10.1080/15980316.2019.1697766},
	doi = {10.1080/15980316.2019.1697766},
	abstract = {Visual discomfort and fatigue due to accommodation-convergence ({AC}) conflict in stereoscopic displays has been widely reported, but little is known about its impact on visual acuity, particularly when automotive three-dimensional (3D) head-up displays ({HUDs}) are involved. This paper presents a study on the visual acuity response when an indigenously developed 75\% transparent retroreflective screen is used as a windshield 3D {HUD}. The simulated optical collimation technique was used to provide the virtual content at a farther depth (i.e. on the road while driving). Two user test experiments were performed. The first test was performed under the see-through condition, where the real scene (i.e. roadside view) was perceived through the 3D {HUD}, while the second test was performed under the simulated collimation condition, where a stereo-collimated virtual content was projected on the {HUD} at a farther depth. The results showed a slightly declining trend (from 20/20 to 20/25) in visual acuity response when the {HUD} screen was placed between the viewer and the scene. An inverse relation between the amount of {AC} conflict and visual acuity was observed under the simulated collimation condition. The {\textgreater}100 cm user-to-screen distance was found to be comfortable, providing the highest acuity response.},
	pages = {93--101},
	number = {2},
	journaltitle = {Journal of Information Display},
	author = {Soomro, Shoaib R. and Urey, Hakan},
	urldate = {2024-12-10},
	date = {2020-04-02},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15980316.2019.1697766},
	keywords = {{AC} conflict, Head-up display, transparent screens, visual acuity, windshield screens},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\UJHJSAH5\\Soomro and Urey - 2020 - Visual acuity response when using the 3D head-up display in the presence of an accommodation-converg.pdf:application/pdf},
}

@article{bayle_interocular_2021,
	title = {Interocular conflict from a monocular augmented reality display: Impact of visual characteristics on performance},
	volume = {16},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0256766},
	doi = {10.1371/journal.pone.0256766},
	shorttitle = {Interocular conflict from a monocular augmented reality display},
	abstract = {In monocular see-through augmented reality systems, each eye is stimulated differently by a monocular image that is superimposed on the binocular background. This can impair binocular fusion, due to interocular conflict. As a function of visual characteristics, the latter can have a greater or lesser impact on user comfort and performance. This study tested several visual characteristics of a binocular background and a monocular element during an exposure that reproduced the interocular conflict induced by a monocular see-through near-eye display. The aim was to identify which factors impact the user’s performance. Performance was measured as target tracking and event detection, identification, fixation time, and latency. Our results demonstrate that performance is a function of the binocular background. Furthermore, exogenous attentional stimulation, in the form of a pulse with different levels of contrast applied to the monocular display, appears to preserve performance in most background conditions.},
	pages = {e0256766},
	number = {9},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Bayle, Elodie and Hourlier, Sylvain and Lelandais, Sylvie and Salasc, Charles-Antoine and Leroy, Laure and Plantier, Justin and Neveu, Pascaline},
	urldate = {2024-12-10},
	date = {2021-09-02},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Attention, Binoculars, Eye movements, Eyes, Fatigue, Luminance, Sensory perception, Vision},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\SQ4ZX2TV\\Bayle et al. - 2021 - Interocular conflict from a monocular augmented reality display Impact of visual characteristics on.pdf:application/pdf},
}

@inproceedings{konrad_novel_2016,
	location = {New York, {NY}, {USA}},
	title = {Novel Optical Configurations for Virtual Reality: Evaluating User Preference and Performance with Focus-tunable and Monovision Near-eye Displays},
	isbn = {978-1-4503-3362-7},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858140},
	doi = {10.1145/2858036.2858140},
	series = {{CHI} '16},
	shorttitle = {Novel Optical Configurations for Virtual Reality},
	abstract = {Emerging virtual reality ({VR}) displays must overcome the prevalent issue of visual discomfort to provide high-quality and immersive user experiences. In particular, the mismatch between vergence and accommodation cues inherent to most stereoscopic displays has been a long standing challenge. In this paper, we evaluate several adaptive display modes afforded by focus-tunable optics or actuated displays that have the promise to mitigate visual discomfort caused by the vergence-accommodation conflict, and improve performance in {VR} environments. We also explore monovision as an unconventional mode that allows each eye of an observer to accommodate to a different distance. While this technique is common practice in ophthalmology, we are the first to report its effectiveness for {VR} applications with a custom built set up. We demonstrate that monovision and other focus-tunable display modes can provide better user experiences and improve user performance in terms of reaction times and accuracy, particularly for nearby simulated distances in {VR}.},
	pages = {1211--1220},
	booktitle = {Proceedings of the 2016 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Konrad, Robert and Cooper, Emily A. and Wetzstein, Gordon},
	urldate = {2024-12-10},
	date = {2016-05-07},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\KW7BFIPG\\Konrad et al. - 2016 - Novel Optical Configurations for Virtual Reality Evaluating User Preference and Performance with Fo.pdf:application/pdf},
}

@online{noauthor_focus_nodate,
	title = {Focus and Ocular Parallax Cues for Virtual and Augmented Reality Displays - {ProQuest}},
	url = {https://www.proquest.com/docview/2432073204?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses},
	abstract = {Immersive computer graphics systems strive to generate perceptually realistic user experiences. Current-generation near-eye displays, such as virtual or augmented reality ({VR}/{AR}) displays, have made significant progress towards this goal with advances in certain technologies: interactive, photorealistic rendering; high-resolution, high-refresh-rate, low-persistence, stereoscopic displays; and low-latency head tracking. Modern {VR} and {AR} systems provide all of these capabilities and create experiences that support many, but not all, of the monocular and binocular depth cues of the human visual system, including occlusions, shading, binocular disparity, and motion parallax. However, at least two depth cues–focus and ocular parallax cues–are missing. In this dissertation, I introduce and evaluate practical systems aimed at creating perceptually realistic experiences by supporting these two cues.},
	urldate = {2024-12-10},
	file = {Focus and Ocular Parallax Cues for Virtual and Augmented Reality Displays - ProQuest:C\:\\Users\\annab\\Zotero\\storage\\82U837P6\\2432073204.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\Q6CKI5BF\\Focus and Ocular Parallax Cues for Virtual and Augmented Reality Displays - ProQuest.pdf:application/pdf},
}

@article{zhou_vergence-accommodation_2021,
	title = {Vergence-accommodation conflict in optical see-through display: review and prospect},
	volume = {5},
	issn = {2666-9501},
	url = {https://www.sciencedirect.com/science/article/pii/S2666950121001061},
	doi = {10.1016/j.rio.2021.100160},
	shorttitle = {Vergence-accommodation conflict in optical see-through display},
	abstract = {The optical see-through near-eye display finds wide applications in various areas such as medical treatment, education, tourism and entertainment. However, the vergence-accommodation conflict ({VAC}) is an obstacle in developing the optical performance of high quality. Conventional stereoscopic displays provide the virtual images in single focal plane, which cannot satisfy the accommodative perception of human eyes. In this review, we elaborate the causes of {VAC}, discuss various methodologies to solve the {VAC} problem and compare the advantages and shortfalls of typical designs. It is concluded that the mandatory software and hardware combining methods for the system is developing trend. A prospect is also given for the future development from the users’ perspective.},
	pages = {100160},
	journaltitle = {Results in Optics},
	shortjournal = {Results in Optics},
	author = {Zhou, Yao and Zhang, Jufan and Fang, Fengzhou},
	urldate = {2024-12-10},
	date = {2021-12-01},
	keywords = {3D geometrical waveguide, Focal plane, Optical see-through display, Vergence-accommodation conflict},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\JH52YSUK\\Zhou et al. - 2021 - Vergence-accommodation conflict in optical see-through display review and prospect.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\annab\\Zotero\\storage\\RLYQUIGA\\S2666950121001061.html:text/html},
}

@online{noauthor_enabling_nodate,
	title = {Enabling Gaze-Contingent Accommodation in Presbyopia Correction and Near-Eye Displays - {ProQuest}},
	url = {https://www.proquest.com/docview/2432829972?fromopenview=true&pq-origsite=gscholar&sourcetype=Dissertations%20&%20Theses},
	abstract = {Humans rely on different depth cues to navigate the world around them. among the most important cues are accommodation (refocus) and vergence (inward and outward rotation). Though accommodation  and vergence are neurally linked processes, there are situations in which they fall out of synchrony; specifically, we address two seemingly unrelated issues: presbyopia and virtual and augmented reality ( / ). In  the former, the crystalline lens within the eye itself loses the ability to accommodate due to it sti ening as  a function of age. This sti ening defines presbyopia and occurs in all humans, with diagnosis typically in  their late s. In the latter case of / , a viewer is forced to fix their accommodation to the single plane  of a display while the display’s stereoscopic image encourages their eyes to verge to arbitrary distances to  achieve a sense of three dimensionality. In both cases, the viewer is no longer able to accommodate naturally.  Traditionally, presbyopia and / optics have been static, making it di cult to approximate the abilities of  the once pliable crystalline lens—inherently trading o acuity, field of view, or stereoacuity in exchange. On  the other hand, a more dynamic, gaze-contingent solution requires focus-tunable lenses and eye trackers to  properly mimic the accommodation reflex. We built and evaluated gaze-contingent optical systems for use in  presbyopia correction and / , with our solutions outperforming traditional ones on both quantitative and  qualitative measures.},
	urldate = {2024-12-10},
	file = {Enabling Gaze-Contingent Accommodation in Presbyopia Correction and Near-Eye Displays - ProQuest:C\:\\Users\\annab\\Zotero\\storage\\6Y3WXTGP\\2432829972.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\LJZSCNZV\\Enabling Gaze-Contingent Accommodation in Presbyopia Correction and Near-Eye Displays - ProQuest.pdf:application/pdf},
}

@article{levi_applications_2023,
	title = {Applications and implications for extended reality to improve binocular vision and stereopsis},
	volume = {23},
	issn = {1534-7362},
	url = {https://doi.org/10.1167/jov.23.1.14},
	doi = {10.1167/jov.23.1.14},
	abstract = {Extended reality ({XR}) devices, including virtual reality ({VR}), augmented reality ({AR}), and mixed reality ({MR}) devices, are immersive technologies that can swap or merge the natural environment with virtual content (e.g., videogames, movies, or other content). Although these devices are widely used for playing videogames and other applications, they have one distinct feature that makes them potentially very useful for the measurement and treatment of binocular vision anomalies—they can deliver different content to the two eyes simultaneously. Indeed, horizontally shifting the images in the two eyes (thereby creating binocular disparity) can provide the user with a compelling percept of depth through stereopsis. Because these devices are stereoscopic, they can also be used as high-tech synoptophores, in which the images to the two eyes differ in contrast, luminance, size, position, and content for measuring and treating binocular anomalies. The inclusion of eye tracking in {VR} adds an additional dimension to its utility in measuring and treating binocular vision anomalies, as well as other conditions. This paper describes the essential requirements for testing and treating binocular anomalies and reviews current studies in which {XR} devices have been used to measure and treat binocular vision anomalies.},
	pages = {14},
	number = {1},
	journaltitle = {Journal of Vision},
	shortjournal = {Journal of Vision},
	author = {Levi, Dennis M.},
	urldate = {2024-12-10},
	date = {2023-01-20},
	file = {Full Text:C\:\\Users\\annab\\Zotero\\storage\\R4DVBKBL\\Levi - 2023 - Applications and implications for extended reality to improve binocular vision and stereopsis.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\ZCXI4499\\article.html:text/html},
}

@article{bhowmik_virtual_2024,
	title = {Virtual and augmented reality: Human sensory-perceptual requirements and trends for immersive spatial computing experiences},
	volume = {32},
	rights = {© 2024 Society for Information Display.},
	issn = {1938-3657},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jsid.2001},
	doi = {10.1002/jsid.2001},
	shorttitle = {Virtual and augmented reality},
	abstract = {Building on several decades of research and development, the recent progress in virtual reality ({VR}) and augmented reality ({AR}) devices with spatial computing technologies marks a significant leap in human–computer interaction, with applications ranging from entertainment and education to e-commerce and healthcare. Advances in these technologies promise immersive experiences by simulating and augmenting the real world with computer-generated digital content. The core objective of the {VR} and {AR} systems is to create convincing human sensory perceptions, thereby creating immersive and interactive experiences that bridge the gap between virtual and physical realities. However, achieving true immersion remains a goal, and it necessitates a comprehensive understanding of the neuroscience of human multisensory perception and accurate technical implementations to create a consistency between natural and synthetic sensory cues. This paper reviews the human sensory-perceptual requirements vital for achieving such immersion, examines the current status and challenges, and discusses potential future advancements.},
	pages = {605--646},
	number = {8},
	journaltitle = {Journal of the Society for Information Display},
	author = {Bhowmik, Achintya K.},
	urldate = {2024-12-10},
	date = {2024},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jsid.2001},
	keywords = {virtual reality, augmented reality, immersive experiences, natural interactions, sensory perception},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\MQMDI4T8\\Bhowmik - 2024 - Virtual and augmented reality Human sensory-perceptual requirements and trends for immersive spatia.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\83NVM2M6\\jsid.html:text/html},
}

@inproceedings{patney_applications_2018,
	location = {New York, {NY}, {USA}},
	title = {Applications of vision science to virtual and augmented reality},
	isbn = {978-1-4503-5809-5},
	url = {https://dl.acm.org/doi/10.1145/3214834.3214851},
	doi = {10.1145/3214834.3214851},
	series = {{SIGGRAPH} '18},
	pages = {1--28},
	booktitle = {{ACM} {SIGGRAPH} 2018 Courses},
	publisher = {Association for Computing Machinery},
	author = {Patney, Anjul and Zannoli, Marina and Kim, Joohwan and Konrad, Robert and Steinicke, Frank and Banks, Martin S.},
	urldate = {2024-12-10},
	date = {2018-08-12},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\95V765ZJ\\Patney et al. - 2018 - Applications of vision science to virtual and augmented reality.pdf:application/pdf},
}

@article{huang_effects_2022,
	title = {Effects of viewing distance and age on the performance and symptoms in a visual search task in augmented reality},
	volume = {102},
	issn = {0003-6870},
	url = {https://www.sciencedirect.com/science/article/pii/S0003687022000692},
	doi = {10.1016/j.apergo.2022.103746},
	abstract = {In augmented reality ({AR}), virtual information is optically combined with the physical environment. In the most frequently used combination technique, optical settings in {AR} depart from the settings in natural viewing. Depending on the combination of viewing distances of the virtual task and its physical background, this deviation may lower visual performance and cause visual disturbance symptoms. The so-called vergence-accommodation conflict ({VAC}) has been identified as a cause for the visual disturbance symptoms in {AR}. In this study, for various distance combinations, the performance and symptoms when performing a search task displayed in a see-through head-mounted display ({AR} {HMD}, {HoloLens} 1st generation, Microsoft, {USA}) was investigated. The search task was displayed at a virtual distance of either 200 cm or 30 cm, and the real background was viewed either at a distance of 200 cm or 30 cm. Three combinations of viewing distances for the background and the virtual task were studied: 200 cm/200 cm, 200 cm/30 cm, and 30 cm/30 cm. Results revealed that both performance and visual disturbance symptoms depend on the combination of the viewing distances of the physical background and the virtual task. When the physical background was viewed at a distance of 200 cm, younger participants showed a significantly better search performance and reported stronger symptoms compared with older participants, no matter whether the virtual task was performed at 30 cm or at 200 cm. However, with the physical background at a distance of 30 cm, the performance of the younger group dropped to the level of the performance of the older group, and younger participants tended to report a stronger increase in visual disturbance symptoms compared with the older participants. From the {AR} {HMD} technology used in this study, it can be concluded that a near viewing distance of the virtual task does not cause a negative impact on performance and visual disturbance symptoms, provided any physical background seen through the {AR} {HMD} is not at a near viewing distance. The findings indicate that the {VAC}, which persists in augmented and virtual reality, depends, in addition to the physical component evaluating the optical distance, on a cognitive component evaluating the perceived distance. {AR} settings should therefore also be evaluated in terms of possible effects on perceived distance.},
	pages = {103746},
	journaltitle = {Applied Ergonomics},
	shortjournal = {Applied Ergonomics},
	author = {Huang, Ying-Yin and Menozzi, Marino},
	urldate = {2024-12-10},
	date = {2022-07-01},
	keywords = {Augmented reality, Accommodation-vergence conflict, Near work, Viewing distance},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\7Q3XF5KD\\Huang and Menozzi - 2022 - Effects of viewing distance and age on the performance and symptoms in a visual search task in augme.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\annab\\Zotero\\storage\\8KMF27GA\\S0003687022000692.html:text/html},
}

@article{zou_eeg-based_2015,
	title = {{EEG}-Based Assessment of Stereoscopic 3D Visual Fatigue Caused by Vergence-Accommodation Conflict},
	volume = {11},
	issn = {1558-9323},
	url = {https://ieeexplore.ieee.org/abstract/document/7140726?casa_token=zmWUygszoUMAAAAA:aB0DIzJ83ft_LxBT6p6sRij9oPvjrClAyzC1VXcjoVcFerbPzq8FbpU84aHadn3zfl3iM8B2},
	doi = {10.1109/JDT.2015.2451087},
	abstract = {Recent advances in 3D displays have contributed to the pressing need of new measurement methods for display comfort. Developing a valid measurement of visual fatigue caused by 3D display remains a big challenge and is beneficial for optimizing the system design. This paper assessed three electroencephalography ({EEG}) activities, θ, α and β, during a monotonous and repetitive random dot stereogram ({RDS}) based task in a conventional stereoscopic 3D display. Six types of ratio indices were computed based on {EEG} data and assessed as possible indicators for stereoscopic visual fatigue detection. The results of critical flicker frequency ({CFF}) and accommodative amplitude ({ACC}) showed that the proposed experiment setup can induce visual fatigue. According to the subjective ratings, the visual fatigue accumulated in this task was mostly related to the binocular vision stress of 3D display. Results of {EEG} data showed stable θ activity, a significant increase of α activity, and a significant decrease of β activity over time . In addition, the effectiveness of {EEG} indices was evaluated to measure stereoscopic visual fatigue by using grey relation analysis ({GRA}) and verified by correlating with {CFF}. The results of analysis suggest that among all nine types of {EEG} indices (θ, α, β, θ/β, α/β, α/θ, θ/(α+β), (α+θ)/β, (α+θ)/(α+β)), α is the most promising indicator for detecting stereoscopic visual fatigue.},
	pages = {1076--1083},
	number = {12},
	journaltitle = {Journal of Display Technology},
	author = {Zou, Bochao and Liu, Yue and Guo, Mei and Wang, Yongtian},
	urldate = {2024-12-10},
	date = {2015-12},
	note = {Conference Name: Journal of Display Technology},
	keywords = {Stereo image processing, Visualization, Fatigue, 3D display, Analysis of variance, Electroencephalography, electroencephalography ({EEG}), Fuses, grey relation analysis, objective measurement, Three-dimensional displays, visual fatigue},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\annab\\Zotero\\storage\\36JRH9Z3\\7140726.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\I7X8M7JV\\Zou et al. - 2015 - EEG-Based Assessment of Stereoscopic 3D Visual Fatigue Caused by Vergence-Accommodation Conflict.pdf:application/pdf},
}

@inproceedings{drascic_perceptual_nodate,
	title = {Perceptual issues in augmented reality},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/2653/0000/Perceptual-issues-in-augmented-reality/10.1117/12.237425.full},
	abstract = {Between the extremes of real life and Virtual Reality lies the spectrum of Mixed Reality, in which views of the real world are combined in some proportion with views of a virtual environment. Combining direct view, stereoscopic video, and stereoscopic graphics, Augmented Reality describes that class of displays that consists primarily of a real environment, with graphic enhancements or augmentations. Augmented Virtuality describes that class of displays that enhance the virtual experience by adding elements of the real environment. All Mixed Reality systems are limited in their capability of accurately displaying and controlled all relevant depth cues, and as a result, perceptual biases can interfere with task performance. In this paper we identify and discuss eighteen issues that pertain to Mixed Reality in general, and Augmented Reality in particular.},
	author = {Drascic, David and Milgram, Paul},
	urldate = {2024-12-10},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\3RS3INFY\\Drascic and Milgram - Perceptual issues in augmented reality.pdf:application/pdf},
}

@article{lin_effects_2022,
	title = {Effects of virtual target size, position, and parallax on vergence-accommodation conflict as estimated by actual gaze},
	volume = {12},
	rights = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-24450-9},
	doi = {10.1038/s41598-022-24450-9},
	abstract = {Due to the increased utilization of stereoscopic displays, the scope of the vergence–accommodation conflict has been studied extensively to reveal how the human visual system operates. The purpose of this work was to study the phenomenon of vergence–accommodation conflict by comparing the theoretical eye vergence angle (vergence response) and gaze-based eye vergence angle (vergence stimulus) based on eye tracker gaze data. The results indicated that the gaze-based eye vergence angle was largest at the greatest parallax. The result also revealed that the eye vergence angle accuracy was significantly highest at the nearest parallax. Generally, accuracy improves when virtual objects are put in the middle and close to participants' positions. Moreover, the signed error decreases significantly when the virtual object is in the middle. Based on the results of this study, we can gain a greater understanding of the vergence–accommodation conflict in the stereoscopic environment.},
	pages = {20100},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Lin, Chiuhsiang Joe and Canny, Susmitha},
	urldate = {2024-12-10},
	date = {2022-11-22},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Fatigue, Eye manifestations, Human behaviour, Risk factors},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\4AHGXLY7\\Lin and Canny - 2022 - Effects of virtual target size, position, and parallax on vergence-accommodation conflict as estimat.pdf:application/pdf},
}

@article{mcanally_visually_2024,
	title = {Visually guided movement in virtual reality is tolerant of the vergence-accommodation conflict},
	volume = {82},
	issn = {0141-9382},
	url = {https://www.sciencedirect.com/science/article/pii/S0141938224000325},
	doi = {10.1016/j.displa.2024.102668},
	abstract = {Stereoscopic virtual reality ({VR}) headsets display vergence cues to object distance but present images at a fixed focus, resulting in a vergence-accommodation conflict ({VAC}). This study examined the effects of introducing or reducing the {VAC} with optical lenses in a targeted reaching task implemented in both {VR} and the real world. Contrary to previous reports of reduced visual performance and fatigue with the {VAC}, we found no evidence of impairments to visually guided movement for either the real-world or {VR}-based versions of the task. As lenses also magnify or minify the image, a control experiment was conducted to examine the effects of image minification on movement in {VR}. No significant effects of optical correction or image minification were found, suggesting that the {VAC} does not adversely affect visually guided movement over the durations employed in this study.},
	pages = {102668},
	journaltitle = {Displays},
	shortjournal = {Displays},
	author = {{McAnally}, Ken and Wallis, Guy and Grove, Philip},
	urldate = {2024-12-10},
	date = {2024-04-01},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\NAUQL6TN\\McAnally et al. - 2024 - Visually guided movement in virtual reality is tolerant of the vergence-accommodation conflict.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\annab\\Zotero\\storage\\PPV8UCE2\\S0141938224000325.html:text/html},
}

@article{lugtenberg_effects_2024,
	title = {Effects of Eye Vergence and Accommodation on Interactions with Content on an {AR} Magic-lens Display and its Surroundings},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/abstract/document/10535764},
	doi = {10.1109/TVCG.2024.3403261},
	abstract = {Augmented reality ({AR}) magic-lens ({ML}) displays, such as handheld devices, offer a convenient and accessible way to enrich our environment using virtual imagery. Several display technologies, including conventional monocular, less common stereoscopic, and varifocal displays, are currently being used. Vergence and accommodation effects on depth perception, as well as vergence–accommodation conflict, have been studied, where users interact only with the content on the display. However, little research exists on how vergence and accommodation influence user performance and cognitive-task load when users interact with the content on a display and its surroundings in a short timeframe. Examples of this are validating augmented instructions before making an incision andperforming general hand-eye coordinated tasks such as grasping augmented objects. To improve interactions with future {AR} displays in such scenarios, we must improve our understanding of this influence. To this end, we conducted two fundamental visual-acuity user studies with 28 and 27 participants, while investigating eye vergence and accommodation distances on four {ML} displays. Our findings show that minimizing the accommodation difference between the display and its surroundings is crucial when the gaze between the display and its surroundings shifts rapidly. Minimizing the difference in vergence is more important when viewing the display and its surroundings as a single context without shifting the gaze. Interestingly, the vergence–accommodation conflict did not significantly affect the cognitive-task load nor play a pivotal role in the accuracy of interactions with {AR} {ML} content and its physical surroundings},
	pages = {1--11},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Lugtenberg, Geert and Copi ˇ c Pucihar, Klen and Kljun, Matjaz and Sawabe, Taishi and Fujimoto, Yuichiro and Kanbara, Masayuki and Kato, Hirokazu},
	urldate = {2024-12-10},
	date = {2024},
	note = {Conference Name: {IEEE} Transactions on Visualization and Computer Graphics},
	keywords = {Stereo image processing, Visualization, Fatigue, Augmented Reality—Human-computer interaction— Video see-through display—Vergence-accommodation, Lenses, Optical imaging, Smart phones, Task analysis},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\6RSF6ANV\\Lugtenberg et al. - 2024 - Effects of Eye Vergence and Accommodation on Interactions with Content on an AR Magic-lens Display a.pdf:application/pdf},
}

@article{wang_liquid_2021,
	title = {Liquid crystal technology for vergence-accommodation conflicts in augmented reality and virtual reality systems: a review},
	volume = {9},
	issn = {2168-0396},
	url = {https://doi.org/10.1080/21680396.2021.1948927},
	doi = {10.1080/21680396.2021.1948927},
	shorttitle = {Liquid crystal technology for vergence-accommodation conflicts in augmented reality and virtual reality systems},
	abstract = {With the help of augmented reality ({AR}) and virtual reality ({VR}) systems, users can receive information and connect with each other via near-eye displays ({NEDs}). However, several challenges need to be addressed, and the optics are the major bottlenecks. Users wearing current {NEDs} typically suffer from vergence-accommodation conflict ({VAC}). Furthermore, people with refractive errors need a pair of prescription lenses to clearly see the virtual image and/or environment. Hence, {VAC}-free {AR}/{VR} systems with vision correction functions should be developed. Furthermore, the {AR}/{VR} systems must be designed with slim form factors. Liquid crystal ({LC}) optical elements with a thin form factor have been demonstrated for light modulation in versatile optical systems; thus, {LC}-based solutions have been proposed for {AR}/{VR} systems in the past decade. Herein, we provide a comprehensive review on the existing literature on {LC}-based optical systems and suggest possible solutions to realize a better {NED} system. This review provides an overview of the state-of-the-art progress of {LC} optics in {AR} and {VR} systems. It focuses on the fundamental optics of {NEDs}, origins of {VAC} and current {LC}-based solutions, {LC} lenses for vision correction function, and the guidelines for solving the two aforementioned challenges using {LC} lenses.},
	pages = {35--64},
	number = {1},
	journaltitle = {Liquid Crystals Reviews},
	author = {Wang, Yu-Jen and Lin, Yi-Hsin},
	urldate = {2024-12-10},
	date = {2021-01-02},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/21680396.2021.1948927},
	keywords = {near-eye display, augmented reality, Liquid crystal, prescription, vergence-accommodation conflict},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\IC6CQ7EB\\Wang and Lin - 2021 - Liquid crystal technology for vergence-accommodation conflicts in augmented reality and virtual real.pdf:application/pdf},
}

@inproceedings{batmaz_effect_2022,
	location = {New York, {NY}, {USA}},
	title = {The Effect of the Vergence-Accommodation Conflict on Virtual Hand Pointing in Immersive Displays},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3502067},
	doi = {10.1145/3491102.3502067},
	series = {{CHI} '22},
	abstract = {Previous work hypothesized that for Virtual Reality ({VR}) and Augmented Reality ({AR}) displays a mismatch between disparities and optical focus cues, known as the vergence and accommodation conflict ({VAC}), affects depth perception and thus limits user performance in 3D selection tasks within arm’s reach (peri-personal space). To investigate this question, we built a multifocal stereo display, which can eliminate the influence of the {VAC} for pointing within the investigated distances. In a user study, participants performed a virtual hand 3D selection task with targets arranged laterally or along the line of sight, with and without a change in visual depth, in display conditions with and without the {VAC}. Our results show that the {VAC} influences 3D selection performance in common {VR} and {AR} stereo displays and that multifocal displays have a positive effect on 3D selection performance with a virtual hand.},
	pages = {1--15},
	booktitle = {Proceedings of the 2022 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Batmaz, Anil Ufuk and Barrera Machuca, Mayra Donaji and Sun, Junwei and Stuerzlinger, Wolfgang},
	urldate = {2024-12-10},
	date = {2022-04-29},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\IAAKCMF8\\Batmaz et al. - 2022 - The Effect of the Vergence-Accommodation Conflict on Virtual Hand Pointing in Immersive Displays.pdf:application/pdf},
}

@article{zabels_ar_2019,
	title = {{AR} Displays: Next-Generation Technologies to Solve the Vergence–Accommodation Conflict},
	volume = {9},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/15/3147},
	doi = {10.3390/app9153147},
	shorttitle = {{AR} Displays},
	abstract = {Augmenting reality ({AR}) holds many benefits in how people perceive information and use it in their workflow or leisure activities. A cohesive {AR} experience has many components; nevertheless, the key is display technologies. The current industry standard for the core solution is still conventional stereoscopy, which has proven to be inadequate for near-work due to the caused vergence–accommodation conflict and the inability to precisely overlay the 3D content on the real world. To overcome this, next-generation technologies have been proposed. While the holographic method holds the highest potential of being the ultimate solution, its current level of maturity is not sufficient to yield a practical product. Consequently, the next solution for near-work-capable {AR} displays will be of another type. {LightSpace} Technologies have developed a static multifocal display architecture based on stacked liquid crystal-based optical diffuser elements and a synchronized high-refresh rate image projector. A stream of 2D image depth planes comprising a 3D scene is projected onto respective physically-separated diffuser elements, causing the viewer to perceive a scene as continuous and having all relevant physical as well as psychological depth cues. A system with six image depth planes yielding 6 cpd resolution and 72° horizontal field-of-view has been demonstrated to provide perceptually continuous accommodation over 3.2 Diopter range. A further optimization by using a conventional image combiner resulted in the compact and practical design of the {AR} display.},
	pages = {3147},
	number = {15},
	journaltitle = {Applied Sciences},
	author = {Zabels, Roberts and Osmanis, Krišs and Narels, Mārtiņš and Gertners, Uģis and Ozols, Ainārs and Rūtenbergs, Kārlis and Osmanis, Ilmārs},
	urldate = {2024-12-10},
	date = {2019-01},
	langid = {english},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {virtual reality, augmented reality, 3D, multifocal display, optical diffuser, vergence–accommodation conflict},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\HZ8AK4J7\\Zabels et al. - 2019 - AR Displays Next-Generation Technologies to Solve the Vergence–Accommodation Conflict.pdf:application/pdf},
}

@article{kramida_resolving_2016,
	title = {Resolving the Vergence-Accommodation Conflict in Head-Mounted Displays},
	volume = {22},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/abstract/document/7226865},
	doi = {10.1109/TVCG.2015.2473855},
	abstract = {The vergence-accommodation conflict ({VAC}) remains a major problem in head-mounted displays for virtual and augmented reality ({VR} and {AR}). In this review, I discuss why this problem is pivotal for nearby tasks in {VR} and {AR}, present a comprehensive taxonomy of potential solutions, address advantages and shortfalls of each design, and cover various ways to better evaluate the solutions. The review describes how {VAC} is addressed in monocular, stereoscopic, and multiscopic {HMDs}, including retinal scanning and accommodation-free displays. Eye-tracking-based approaches that do not provide natural focal cues-gaze-guided blur and dynamic stereoscopy-are also covered. Promising future research directions in this area are identified.},
	pages = {1912--1931},
	number = {7},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Kramida, Gregory},
	urldate = {2024-12-10},
	date = {2016-07},
	note = {Conference Name: {IEEE} Transactions on Visualization and Computer Graphics},
	keywords = {Stereo image processing, Vergence-accommodation conflict, Three-dimensional displays, Lenses, Optical imaging, display, eye tracking, freeform prism, head-mounted, Head-Mounted Displays, light field, maxwellian view, Mirrors, multifocal plane, multiscopic, pinlight, Retina, retinal, scanned fiber},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\annab\\Zotero\\storage\\DF6VS8I6\\7226865.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\DUGYXDPV\\Kramida - 2016 - Resolving the Vergence-Accommodation Conflict in Head-Mounted Displays.pdf:application/pdf},
}

@inproceedings{avveduto_real-world_2017,
	location = {New York, {NY}, {USA}},
	title = {Real-world occlusion in optical see-through {AR} displays},
	isbn = {978-1-4503-5548-3},
	url = {https://doi.org/10.1145/3139131.3139150},
	doi = {10.1145/3139131.3139150},
	series = {{VRST} '17},
	abstract = {In this work we describe a system composed by an optical see-through {AR} headset---a Microsoft {HoloLens}---, stereo projectors and shutter glasses. Projectors are used to add to the device the capability of occluding real-world surfaces to make the virtual objects to appear more solid and less transparent.A framework was developed in order to allow us to evaluate the importance of occlusion capabilities in optical see-through {AR} headset. We designed and conducted two experiment to test whether making virtual elements solid would improve the performance of certain tasks with an {AR} system. Results suggest that making virtual objects to appear more solid by projecting an occlusion mask onto the real-world is useful in some situations.Using an occlusion mask it is also possible to eliminate ambiguities that could arise when enhancing user's perception in some ways that are not possible in real-life, like when a "x-ray vision" is enabled. In this case we wanted to investigate if using an occlusion mask to eliminate perceptual conflicts will hit user's performance in some {AR} applications.The framework that allowed us to conduct our experiments is made freely available to anyone interested in conducting future studies.},
	pages = {1--10},
	booktitle = {Proceedings of the 23rd {ACM} Symposium on Virtual Reality Software and Technology},
	publisher = {Association for Computing Machinery},
	author = {Avveduto, Giovanni and Tecchia, Franco and Fuchs, Henry},
	urldate = {2024-12-16},
	date = {2017-11-08},
}

@inproceedings{swan_perceptual_2006,
	title = {A Perceptual Matching Technique for Depth Judgments in Optical, See-Through Augmented Reality},
	url = {https://ieeexplore.ieee.org/abstract/document/1667622},
	doi = {10.1109/VR.2006.13},
	abstract = {A fundamental problem in optical, see-through augmented reality ({AR}) is characterizing how it affects the perception of spatial layout and depth. This problem is important because {AR} system developers need to both place graphics in arbitrary spatial relationships with real-world objects, and to know that users will perceive them in the same relationships. Furthermore, {AR} makes possible enhanced perceptual techniques that have no real-world equivalent, such as x-ray vision, where {AR} users are supposed to perceive graphics as being located behind opaque surfaces. This paper reviews and discusses techniques for measuring egocentric depth judgments in both virtual and augmented environments. It then describes a perceptual matching task and experimental design for measuring egocentric {AR} depth judgments at medium- and far-field distances of 5 to 45 meters. The experiment studied the effect of field of view, the x-ray vision condition, multiple distances, and practice on the task. The paper relates some of the findings to the well-known problem of depth underestimation in virtual environments, and further reports evidence for a switch in bias, from underestimating to overestimating the distance of {AR}-presented graphics, at 23 meters. It also gives a quantification of how much more difficult the x-ray vision condition makes the task, and then concludes with ideas for improving the experimental methodology.},
	eventtitle = {{IEEE} Virtual Reality Conference ({VR} 2006)},
	pages = {19--26},
	booktitle = {{IEEE} Virtual Reality Conference ({VR} 2006)},
	author = {Swan, J.E. and Livingston, M.A. and Smallman, H.S. and Brown, D. and Baillot, Y. and Gabbard, J.L. and Hix, D.},
	urldate = {2024-12-16},
	date = {2006-03},
	note = {{ISSN}: 2375-5334},
	keywords = {Augmented reality, Depth Perception, Displays, Experimentation, Eyes, Graphics, Hardware, Humans, Layout, Measurement, Optical See-Through Augmented Reality, Performance, Retina, Virtual reality, Visual system},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\annab\\Zotero\\storage\\M4I5WIRS\\1667622.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\K5LEL2HB\\Swan et al. - 2006 - A Perceptual Matching Technique for Depth Judgments in Optical, See-Through Augmented Reality.pdf:application/pdf},
}

@incollection{livingston_pursuit_2013,
	title = {Pursuit of “X-Ray Vision” for Augmented Reality {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-1-4614-4205-9_4},
	abstract = {One of the most intriguing capabilities envisioned for augmented reality ({AR}) systems is the notion of “X-ray vision,” or the ability to virtually “see through” one surface to what in reality is hidden from view. Many {AR} systems have been premised on this feature for the primary value provided by the application. Furness’ pioneering work [15, 16] on head-up displays for pilots was motivated in large part by the ability to virtually see through the solid cockpit walls and floor to a virtual copy of the real world hidden by the aircraft infrastructure. The ultrasound {AR} system [2] provided the ability to understand the data acquired by the ultrasound probe from inside the body; to do this requires a metaphor that ensures the user will understand the data to reside behind the visible skin surface, and in fact this was a problem noted early in the development of the system. We believe that the metaphor “X-ray vision” was first applied to this work [45]. There are two sides to this unique perceptual capability. There is the issue of the absolute distance of graphical entities within the coordinate system of the real environment. There is also the relative order of real and virtual surfaces within the merged environment. Neither of these perceptual capabilities seem to come naturally for {AR} users, and thus numerous visual metaphors have been conceived in order to give the impression of relative and absolute depth. Another issue that challenges the designer of X-ray visualization metaphors is the potential to overload the user with information [20, 28]...},
	pages = {67--107},
	booktitle = {Human Factors in Augmented Reality Environments},
	author = {Livingston, Mark and Dey, Arindam and Sandor, Christian and Thomas, Bruce H.},
	urldate = {2024-12-17},
	date = {2013},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\J7W63TWD\\Pursuit of “X-Ray Vision” for Augmented Reality  SpringerLink.pdf:application/pdf;Pursuit of “X-Ray Vision” for Augmented Reality | SpringerLink:C\:\\Users\\annab\\Zotero\\storage\\IGBHQ3E8\\978-1-4614-4205-9_4.html:text/html},
}

@article{au_natural_2024,
	title = {Natural size-distance scaling reduces, but does not eliminate, depth matching errors from conflicting occlusion and stereopsis},
	volume = {24},
	issn = {1534-7362},
	url = {https://doi.org/10.1167/jov.24.10.1392},
	doi = {10.1167/jov.24.10.1392},
	abstract = {Stereoscopic depth matching is significantly degraded when occlusion information conflicts with depth from binocular disparity. However, in these studies the visual angle of the target was held constant while in natural viewing image size changes linearly with object distance. Thus, it is not clear whether the disruption in performance can be solely attributed to the discrepant occlusion and disparity signals. Here we evaluated the combined effects of size, occlusion and binocular disparity using a depth matching paradigm in mixed-reality. The virtual stimulus was a green letter ‘A’ presented using an augmented reality display. It was superimposed on a physical surface with variable transparency fixed at 1.2 m. The target letter was placed at one of eight distances (0.9 - 1.6 m), including the surface location. The letter was rendered either with a fixed size (variable retinal angle) or with size scaled to maintain a constant visual angle. Observers matched the distance of a virtual probe to the perceived distance of the letter in three conditions where the surface was: opaque, transparent or absent. We found that depth matches were accurate and there was no effect of size scaling in both the ‘no surface’ and ‘transparent surface’ conditions. This was also the case when the letter appeared in front of the surface. However, when the letter was positioned beyond the opaque surface (maximum cue conflict) its position was systematically underestimated. Introducing correct size scaling reduced this error but did not eliminate it. In sum, when occlusion and disparity are in conflict our results show that using a fixed retinal size exacerbates depth matching errors. When retinal size varies (as in the natural world) depth matching is more accurate but significant underestimates remain. When occlusion and disparity signals are in agreement, size has little impact on performance.},
	pages = {1392},
	number = {10},
	journaltitle = {Journal of Vision},
	shortjournal = {Journal of Vision},
	author = {Au, Domenic and Allison, Robert and Wilcox, Laurie},
	urldate = {2024-12-17},
	date = {2024-09-15},
	file = {DomenicAu_VSSposter_2024:C\:\\Users\\annab\\Zotero\\storage\\A2K9JKKA\\DomenicAu_VSSposter_2024.pdf:application/pdf;PDF:C\:\\Users\\annab\\Zotero\\storage\\US3LLJD5\\Au et al. - 2024 - Natural size-distance scaling reduces, but does not eliminate, depth matching errors from conflictin.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\4IX5RD46\\article.html:text/html},
}

@article{macedo_occlusion_2023,
	title = {Occlusion Handling in Augmented Reality: Past, Present and Future},
	volume = {29},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/abstract/document/9560081},
	doi = {10.1109/TVCG.2021.3117866},
	shorttitle = {Occlusion Handling in Augmented Reality},
	abstract = {One of the main goals of many augmented reality applications is to provide a seamless integration of a real scene with additional virtual data. To fully achieve that goal, such applications must typically provide high-quality real-world tracking, support real-time performance and handle the mutual occlusion problem, estimating the position of the virtual data into the real scene and rendering the virtual content accordingly. In this survey, we focus on the occlusion handling problem in augmented reality applications and provide a detailed review of 161 articles published in this field between January 1992 and August 2020. To do so, we present a historical overview of the most common strategies employed to determine the depth order between real and virtual objects, to visualize hidden objects in a real scene, and to build occlusion-capable visual displays. Moreover, we look at the state-of-the-art techniques, highlight the recent research trends, discuss the current open problems of occlusion handling in augmented reality, and suggest future directions for research.},
	pages = {1590--1609},
	number = {2},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Macedo, Márcio C. F. and Apolinário, Antônio L.},
	urldate = {2024-12-17},
	date = {2023-02},
	note = {Conference Name: {IEEE} Transactions on Visualization and Computer Graphics},
	keywords = {augmented reality, Augmented reality, computational displays, Computer graphics, Data visualization, depth maps, Market research, mutual occlusion, Real-time systems, Search problems, Visualization, X-ray imaging, X-ray vision},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\annab\\Zotero\\storage\\TK4DIY9C\\9560081.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\CUUFQESB\\Macedo and Apolinário - 2023 - Occlusion Handling in Augmented Reality Past, Present and Future.pdf:application/pdf},
}

@article{breen_interactive_nodate,
	title = {Interactive Occlusion and Collision of Real and Virtual Objects in Augmented Reality},
	abstract = {We present several techniques for interactively performing occlusion and collision detection between static real objects and dynamic virtual objects in augmented reality. Computer vision algorithms are used to acquire data that model aspects of the real world. Either geometric models may be registered to real objects, or a depth map of the real scene may be extracted with computer vision algorithms. The computer vision-derived data are mapped into algorithms that exploit the power of graphics workstations, in order to interactively produce new effects in augmented reality. By combining live video from a calibrated camera with real-time renderings of the real-world data from graphics hardware, dynamic virtual objects occlude and are occluded by static real objects. As a virtual object is interactively manipulated collisions with real objects are detected, and the motion of the virtual object is constrained. Simulated gravity may then be produced by automatically moving the virtual object in the direction of a gravity vector until it encounters a collision with a real object.},
	author = {Breen, David E and Rose, Eric and Whitaker, Ross T},
	langid = {english},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\5763GWV7\\Breen et al. - Interactive Occlusion and Collision of Real and Virtual Objects in Augmented Reality.pdf:application/pdf},
}

@inproceedings{singh_depth_2010,
	location = {New York, {NY}, {USA}},
	title = {Depth judgment measures and occluding surfaces in near-field augmented reality},
	isbn = {978-1-4503-0248-7},
	url = {https://doi.org/10.1145/1836248.1836277},
	doi = {10.1145/1836248.1836277},
	series = {{APGV} '10},
	abstract = {In this paper we describe an apparatus and experiment that measured depth judgments in augmented reality at near-field distances of 34 to 50 centimeters. The experiment compared perceptual matching, a closed-loop task for measuring depth judgments, with blind reaching, a visually open-loop task for measuring depth judgments. The experiment also studied the effect of a highly salient occluding surface appearing behind, coincident with, and in front of a virtual object. The apparatus and closed-loop matching task were based on previous work by Ellis and Menges. The experiment found maximum average depth judgment errors of 5.5 cm, and found that the blind reaching judgments were less accurate than the perceptual matching judgments. The experiment found that the presence of a highly-salient occluding surface has a complicated effect on depth judgments, but does not lead to systematically larger or smaller errors.},
	pages = {149--156},
	booktitle = {Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualization},
	publisher = {Association for Computing Machinery},
	author = {Singh, Gurjot and Swan, J. Edward and Jones, J. Adam and Ellis, Stephen R.},
	urldate = {2024-12-16},
	date = {2010-07-23},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\26XDZJ7Q\\Singh et al. - 2010 - Depth judgment measures and occluding surfaces in near-field augmented reality.pdf:application/pdf},
}

@article{tian_real-time_2010,
	title = {Real-Time Occlusion Handling in Augmented Reality Based on an Object Tracking Approach},
	volume = {10},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/10/4/2885},
	doi = {10.3390/s100402885},
	abstract = {To produce a realistic augmentation in Augmented Reality, the correct relative positions of real objects and virtual objects are very important. In this paper, we propose a novel real-time occlusion handling method based on an object tracking approach. Our method is divided into three steps: selection of the occluding object, object tracking and occlusion handling. The user selects the occluding object using an interactive segmentation method. The contour of the selected object is then tracked in the subsequent frames in real-time. In the occlusion handling step, all the pixels on the tracked object are redrawn on the unprocessed augmented image to produce a new synthesized image in which the relative position between the real and virtual object is correct. The proposed method has several advantages. First, it is robust and stable, since it remains effective when the camera is moved through large changes of viewing angles and volumes or when the object and the background have similar colors. Second, it is fast, since the real object can be tracked in real-time. Last, a smoothing technique provides seamless merging between the augmented and virtual object. Several experiments are provided to validate the performance of the proposed method.},
	pages = {2885--2900},
	number = {4},
	journaltitle = {Sensors},
	author = {Tian, Yuan and Guan, Tao and Wang, Cheng},
	urldate = {2024-12-17},
	date = {2010-04},
	langid = {english},
	note = {Number: 4
Publisher: Molecular Diversity Preservation International},
	keywords = {augmented reality, graph cuts, mean shift, occlusion, optical flow, tracking},
	file = {Full Text PDF:C\:\\Users\\annab\\Zotero\\storage\\KRN68KEI\\Tian et al. - 2010 - Real-Time Occlusion Handling in Augmented Reality Based on an Object Tracking Approach.pdf:application/pdf},
}

@article{tian_automatic_2010,
	title = {An automatic occlusion handling method in augmented reality},
	volume = {30},
	issn = {0260-2288},
	url = {https://doi.org/10.1108/02602281011051399},
	doi = {10.1108/02602281011051399},
	abstract = {Purpose – To make an augmented image realistic, the virtual objects should be correctly occluded by foreground objects. The purpose of this paper is to propose a new approach that resolves occlusion problems in augmented reality ({AR}). The main interest is that it can automatically obtain the proper spatial relationship between virtual and real objects in real time. Design/methodology/approach – The approach is divided into two steps: off‐line disparity map constructing and on‐line occlusion handling. In the off‐line stage, the disparity map of the real scene is constructed using the global stereo matching method prior and then the disparities are refined by means of the fast mean shift method. Since the depth values of objects in different positions are different, the real object that occludes the virtual object can be specified according to the depth value. In the on‐line stage, the contour of the specified object is tracked using the real time object tracking method with the combination of feature tracking method and minimum s‐t cut method. The augmented image with correct occlusions is produced by redrawing all the tracked object pixels on the augmented image. Findings – Compared with the existing methods, the proposed approach can automatically resolve occlusion problem in real time. The effectiveness of the method is demonstrated with several experimental results. Originality/value – This paper makes three contributions. First, a novel framework is proposed to handle occlusion problem in {AR}. This framework is different to the previously proposed methods. The main procedure includes: obtain occluding real object, track the object, and redraw the pixels of the object on the composed image. It is much easier to implement and can achieve satisfactory results. Second, the disparity map is used to automatically obtain the contour of the occluding real object. To get the contour of the occluding real object precisely, the mean shift method is used to refine the disparity map. By comparing the depth value, the occluding real object can be extracted automatically. Third, the tracking method combining feature tracking method and minimum s‐t cut method ensures the real‐time requirement. The occlusion problem can be handled in real‐time.},
	pages = {210--218},
	number = {3},
	journaltitle = {Sensor Review},
	author = {Tian, Yuan and Guan, Tao and Wang, Cheng},
	urldate = {2024-12-17},
	date = {2010-01-01},
	note = {Publisher: Emerald Group Publishing Limited},
	keywords = {Computer software, Reality, Tracking},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\Q3DU6S82\\Tian et al. - 2010 - An automatic occlusion handling method in augmented reality.pdf:application/pdf;Snapshot:C\:\\Users\\annab\\Zotero\\storage\\Q47E4WGK\\html.html:text/html},
}

@article{sizintsev_long-range_2021,
	title = {Long-Range Augmented Reality with Dynamic Occlusion Rendering},
	volume = {27},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/abstract/document/9523845},
	doi = {10.1109/TVCG.2021.3106434},
	abstract = {Proper occlusion based rendering is very important to achieve realism in all indoor and outdoor Augmented Reality ({AR}) applications. This paper addresses the problem of fast and accurate dynamic occlusion reasoning by real objects in the scene for large scale outdoor {AR} applications. Conceptually, proper occlusion reasoning requires an estimate of depth for every point in augmented scene which is technically hard to achieve for outdoor scenarios, especially in the presence of moving objects. We propose a method to detect and automatically infer the depth for real objects in the scene without explicit detailed scene modeling and depth sensing (e.g. without using sensors such as 3D-{LiDAR}). Specifically, we employ instance segmentation of color image data to detect real dynamic objects in the scene and use either a top-down terrain elevation model or deep learning based monocular depth estimation model to infer their metric distance from the camera for proper occlusion reasoning in real time. The realized solution is implemented in a low latency real-time framework for video-see-though {AR} and is directly extendable to optical-see-through {AR}. We minimize latency in depth reasoning and occlusion rendering by doing semantic object tracking and prediction in video frames.},
	pages = {4236--4244},
	number = {11},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Sizintsev, Mikhail and Mithun, Niluthpol Chowdhury and Chiu, Han-Pang and Samarasekera, Supun and Kumar, Rakesh},
	urldate = {2024-12-17},
	date = {2021-11},
	note = {Conference Name: {IEEE} Transactions on Visualization and Computer Graphics},
	keywords = {Augmented reality, Cameras, Cognition, depth inference, Estimation, Image segmentation, Navigation, object tracking, occlusion reasoning, Real-time systems, Rendering (computer graphics)},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\annab\\Zotero\\storage\\DVVTWZ5Q\\9523845.html:text/html;PDF:C\:\\Users\\annab\\Zotero\\storage\\KUJAALIQ\\Sizintsev et al. - 2021 - Long-Range Augmented Reality with Dynamic Occlusion Rendering.pdf:application/pdf},
}

@online{noauthor_checkout_nodate,
	title = {Checkout - Nécessaire, A Personal Care Company},
	url = {https://shop.app/checkout/3488120947/cn/Z2NwLXVzLWNlbnRyYWwxOjAxSkZHV1EzNkY1RVk4NFIySjZWQktBMlFO/shoppay?redirect_source=checkout_automatic_redirect&tracking_unique=b5f5f8df-fdc3-49fc-9476-777afe401f8d&tracking_visit=8a45b042-251e-4b04-8f94-8a3dccbe57c5},
	urldate = {2024-12-20},
	file = {Checkout - Nécessaire, A Personal Care Company:C\:\\Users\\annab\\Zotero\\storage\\7K8VH8MQ\\shoppay.html:text/html},
}

@online{stix_see-through_nodate,
	title = {See-through view: Virtual reality may guide physician’s hands.},
	url = {https://ui.adsabs.harvard.edu/abs/1992SciAm.267c.166S/abstract},
	author = {Stix, G},
	urldate = {2024-12-20},
	file = {See-Through View - ADS:C\:\\Users\\annab\\Zotero\\storage\\RF6CIDFX\\abstract.html:text/html},
}

@article{fuhrmann_occlusion_nodate,
	title = {Occlusion in collaborative augmented environments},
	abstract = {Augmented environments superimpose computer enhancements on the real world. Such augmented environments are well suited for collaboration of multiple users. To improve the quality and consistency of the augmentation the occlusion of real objects by computer-generated objects and vice versa has to be implemented. We present methods how this can be done for a tracked user's body and other real objects and how irritating artifacts due to misalignments can be reduced. Our method is based on simulating the occlusion of virtual objects by a representation of the user modeled as kinematic chains of articulated solids. Smoothing the border between virtual world and occluding real reduces registration and modeling errors of this model. Finally, an implementation in our augmented environment and the resulting improvements are presented. ( 1999 Elsevier Science Ltd. All rights reserved.},
	author = {Fuhrmann, Anton and Hesina, Gerd},
	langid = {english},
	file = {PDF:C\:\\Users\\annab\\Zotero\\storage\\HAE77FPN\\Fuhrmann and Hesina - Occlusion in collaborative augmented environments.pdf:application/pdf},
}
